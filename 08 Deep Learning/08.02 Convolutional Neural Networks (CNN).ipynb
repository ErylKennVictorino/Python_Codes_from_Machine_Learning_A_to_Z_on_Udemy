{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AUTHOR**:     ERYL KENN VICTORINO  \n",
    "**PURPOSE**:    CONVOLUTIONAL NEURAL NETWORKS (CNN) TUTORIAL  \n",
    "*from 'Machine Learning A-Zâ„¢: Hands-On Python & R In Data Science' on Udemy by Kirill Eremenko, Hadelin de Ponteves, and the SuperDataScience Team*  \n",
    "**MOD DATE**:   4/20/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: BUILD CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZE CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1:     CONVOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*start with 32 feature detectors of side 3x3  \n",
    "colored images become 3D arrays and black and white images become 2D arrays  \n",
    "since our images are colored and to save time, use 3 and a 64x64 image format for \"input_shape\"  \n",
    "remember that the tensorflow backend has a different parameter order for \"input_shape\"  \n",
    "use a rectifier \"relu\" activation function to ensure non-linearity*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = \"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2:     POOLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*apply \"pooling\" to reduce feature map size and reduce future number of nodes  \n",
    "use \"max\" pooling to ensure feature detection without losing performance  \n",
    "use a 2x2 \"pool_size\" in general*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD ANOTHER CONVOLUTIONAL LAYER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*to improve the accuracy of your model, either add another convolutional layer, another fully-connected layer or both*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(32, (3, 3), activation = \"relu\"))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3:     FLATTENING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4:     FULL CONNECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*choose 128 \"units\" for the number of nodes in the hidden layer  \n",
    "use a rectifier (\"relu\") activation function for the hidden layer  \n",
    "choose 1 \"units\" for the output layer  \n",
    "use a sigmoid activation function for the output layer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(activation=\"relu\", units=128))\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPILE CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*use \"adam\" stochastic gradient descent for optimizer  \n",
    "use \"binary_crossentropy\" loss function since it is logarithmic and has a binary outcome  \n",
    "use \"categorical_crossentropy\" if it is more than two categories  \n",
    "use the \"accuracy\" criterion to evaluate the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: CNN FIT ON IMAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*the following code cells example was taken from the keras website for image preprocessing https://keras.io/preprocessing/image/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*remember to change the directory to where your samples are  \n",
    "match the target size to the image format from CNN (in this case 64x64)  \n",
    "use 32 for the batch size  \n",
    "since we are predicting if an image is a cat or a dog, use a \"binary\" class mod*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(\"dataset/training_set\",\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = \"binary\")\n",
    "test_set = test_datagen.flow_from_directory(\"dataset/test_set\",\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = \"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*use the number of samples in your training set for \"samples_per_epoch\"  \n",
    "use the number of samples in your test set for \"nb_val_samples\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 231s 925ms/step - loss: 0.6785 - accuracy: 0.5673 - val_loss: 0.6608 - val_accuracy: 0.6405\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 51s 205ms/step - loss: 0.6085 - accuracy: 0.6730 - val_loss: 0.5903 - val_accuracy: 0.6910\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 0.5484 - accuracy: 0.7196 - val_loss: 0.5217 - val_accuracy: 0.7490\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 49s 195ms/step - loss: 0.5144 - accuracy: 0.7496 - val_loss: 0.5590 - val_accuracy: 0.7260\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 50s 200ms/step - loss: 0.4959 - accuracy: 0.7599 - val_loss: 0.4974 - val_accuracy: 0.7620\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 51s 202ms/step - loss: 0.4685 - accuracy: 0.7786 - val_loss: 0.4732 - val_accuracy: 0.7860\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 50s 199ms/step - loss: 0.4517 - accuracy: 0.7901 - val_loss: 0.4637 - val_accuracy: 0.7950\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 52s 207ms/step - loss: 0.4329 - accuracy: 0.7974 - val_loss: 0.5080 - val_accuracy: 0.7680\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 52s 210ms/step - loss: 0.4240 - accuracy: 0.7986 - val_loss: 0.4770 - val_accuracy: 0.7845\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 50s 202ms/step - loss: 0.4093 - accuracy: 0.8108 - val_loss: 0.5145 - val_accuracy: 0.7660\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 51s 202ms/step - loss: 0.3914 - accuracy: 0.8160 - val_loss: 0.5250 - val_accuracy: 0.7780\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 52s 209ms/step - loss: 0.3790 - accuracy: 0.8276 - val_loss: 0.4886 - val_accuracy: 0.7935\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 52s 209ms/step - loss: 0.3718 - accuracy: 0.8311 - val_loss: 0.4585 - val_accuracy: 0.8105\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 0.3555 - accuracy: 0.8397 - val_loss: 0.4718 - val_accuracy: 0.8045\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 50s 199ms/step - loss: 0.3420 - accuracy: 0.8470 - val_loss: 0.4531 - val_accuracy: 0.8115\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 50s 199ms/step - loss: 0.3324 - accuracy: 0.8550 - val_loss: 0.4492 - val_accuracy: 0.8205\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 52s 208ms/step - loss: 0.3199 - accuracy: 0.8564 - val_loss: 0.4773 - val_accuracy: 0.8085\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 53s 211ms/step - loss: 0.3059 - accuracy: 0.8665 - val_loss: 0.4801 - val_accuracy: 0.8095\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 51s 204ms/step - loss: 0.2925 - accuracy: 0.8741 - val_loss: 0.4941 - val_accuracy: 0.7985\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 52s 207ms/step - loss: 0.2720 - accuracy: 0.8861 - val_loss: 0.4922 - val_accuracy: 0.8120\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 0.2610 - accuracy: 0.8913 - val_loss: 0.5199 - val_accuracy: 0.8025\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 51s 205ms/step - loss: 0.2566 - accuracy: 0.8871 - val_loss: 0.5121 - val_accuracy: 0.8210\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 51s 204ms/step - loss: 0.2422 - accuracy: 0.8970 - val_loss: 0.5072 - val_accuracy: 0.8160\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 53s 211ms/step - loss: 0.2293 - accuracy: 0.9078 - val_loss: 0.5054 - val_accuracy: 0.8190\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 53s 213ms/step - loss: 0.2276 - accuracy: 0.9060 - val_loss: 0.5713 - val_accuracy: 0.8015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x145e3d3ceb8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 8000/32,\n",
    "                         epochs = 25,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 2000/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
